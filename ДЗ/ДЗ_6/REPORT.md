# Отчёт по домашнему заданию: Генератор текста на Transformer

## Цель

Создать генератор текста с использованием декодера Transformer и обучить его на русском тексте.

---

## Что сделано

### 1. Архитектура

- Реализовала модель `GeneratorTransformer` с декодером Transformer.
- Включены позиционные и токеновые эмбеддинги, маскирование будущих токенов.

### 2. Токенизация

- Использован готовый BPE-токенизатор `mistral_tokenizer.json`.
- Добавлены специальные токены BOS и EOS.

### 3. Данные

- Для обучения использованы произведения Ф. М. Достоевского (примерно 1 млн символов).
- Разбила текст на отрезки длиной `max_length=128`

### 4. Обучение

- Обучение шло на CPU.
- Гиперпараметры: `batch_size=1`, `epochs=20`, `lr=1e-4`.
- Loss снижался от ~6.7 до ~0.5 к 19-й эпохе.
- Модель сохранялась после каждой эпохи.

### 5. Генерация

- Реализован метод `generate`: по введённому тексту модель предсказывает продолжение.
- Контекст сдвигается на 1 токен на каждом шаге.
- Добавлен `beam search` для улучшения качества генерации.

### 6. Интерфейс

generate.py

---

## Результат

- Модель успешно обучена и умеет генерировать тексты в стиле Достоевского (она пытается)))).
- Beam search даёт более связные и осмысленные ответы.
